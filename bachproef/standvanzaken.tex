\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

In de digitale wereld van vandaag is data de sleutel tot succes. Bedrijven verzamelen en ana-lyseren data om hun klanten beter te begrijpen, hun marketingstrategieën te optimaliseren, en hun concurrentiepositie te versterken \autocite{},
Deze enorme vraag naar kwalitatieve data drijftde ontwikkeling van nieuwe en innovatieve me-thodes voor dataverzameling. Er zijn vele manie-ren waarop er aan dataverzameling kan gedaan worden, deze studie focussed zich op webscra-ping.

\section{Webscraping}
Webscraping is een techniek die automatisch
data extraheert van websites. Deze data kan in
verschillende formaten voorkomen, zoals HTML,
JSON, XML en CSV. Webscraping heeft tal van toepassingen,
waaronder:
\begin{itemize}
    \item Data-extractie, dit is het verzamelen van data van websites voor diverse doeleinden,
    zoals marktonderzoek,prijsvergelijking en het verzamelen van recensies
    \item Het automatiseren van taken die anders handmatig zouden moeten worden uitgevoerd, denk maar
    aan het invullen van bepaalde formulieren, aanmaken van documenten en downloaden van bestanden
    \item Monitoren van websites voor veranderingen van prijzen, nieuwe producten of statussen van bepaalde zaken
\end{itemize}

\subsection{Traditionele webscraping}

In dit hoofdstuk wordt traditionele webscraping  onderzocht als een techniek voor het extraheren van data van websites door het parsen van de HTML-code van websites. De focus ligt op het begrijpen van de werking van traditionele webscraping, het identificeren van de voor- en nadelen ervan, en het verkennen van enkele alternatieve methoden.

Traditionele webscraping begint met het identificeren van de doelpagina die de gewenste data bevat. Vervolgens wordt de HTML-code van de doelpagina opgehaald via een HTTP-request. Deze stap kan worden uitgevoerd met de developer tools  in de browser, Postman of met de een van de meest gebruikte libraries van Python: requests \autocite{Nate2023}.  Na het verkrijgen van de HTML-code wordt deze geparsd om er de gewenste gegevens uit te halen. Dit kan gedaan worden met de Python library BeautifulSoup. De verkregen data kan daarna worden omgezet in verschillende data formaten zoals JSON, CSV en XML of het kan rechtstreeks  in een database of een ander bestandssysteem gestoken worden.

Enkele voordelen van de traditionele webscraping manier zijn:
\begin{itemize}
    \item Eenvoudig: Traditionele webscraping is een van de eenvoudigere methodes die met weinig technische kennis kan worden toegepast. Deze zijn eenvoudig in de zin dat je met zeer weinig code al een basis scraper kan bouwen.
    \item Repliceerbaarheid: De manier waarop de website wordt binnengehaald in deze traditionele webscrapers is voor iedere website hetzelfde. Dit zorgt ervoor dat deze scrapers makkelijk te hergebruiken zijn.
\end{itemize}

Deze eenvoudigheid komt echter niet zonder kost, hier zijn de nadelen van traditionele webscrapers:

\begin{itemize}
    \item Beperkte functionaliteit: Traditionele webscraping is beperkt tot het extraheren van data die op de website is gepresenteerd. Het is niet mogelijk om data te extraheren die dynamisch wordt gegenereerd of die is opgeslagen in databases.
    \item Onderhoud: Websites en dus ook de HTML-code veranderen voortdurend, waardoor webscraping scripts regelmatig moeten worden bijgewerkt zodat ze hun correcte werking behouden.
    \item Parsen: Wanneer de HTML structuur complex en/of niet consistent is kan dit voor problemen zorgen wanneer de HTML-code geparset wordt. Als de HTML structuur niet consistent is zorgt dit ervoor dat de data waarnaar men zoekt misschien niet in dezelfde tag staat als op een vorige pagina van de zelfde website. Het kan ook zijn dat de tag waarin de data staat niet onder dezelfde parent-tag staat.  (insert example)
    \item Efficiëntie: De traditionele webscraping methode haalt de volledige webpagina binnen. Dit wil zeggen dat voor een webscraper die de prijs van één artikel ophaalt, de volledige webpagina wordt geëxtraheerd. Bij grote websites met veel webpagina's zorgt dit op 2 plaatsen voor vertraging. Eerst moet deze volledige webpagina opgehaald worden, daarna wordt deze geparset en dan pas dan kan men opzoek naar de data.
\end{itemize}

Een alternatieve manier om aan webscraping te doen is aan de hand van API's. Websites bieden soms API's (Application Programming Interfaces) aan die het mogelijk maken om data te verzamelen in een gestructureerd formaat\autocite{Ayushi2021}. API's zijn ontworpen voor developers en bieden een gestandaardiseerde manier om data te communiceren.

\subsection{API webscraping}

Een van de belangrijkste voordelen van API's is hun gebruiksgemak. Ze bieden een gestandaardiseerde interface waarmee gebruikers eenvoudig toegang kunnen krijgen tot de gewenste gegevens. Dit maakt het proces van gegevensverzameling efficiënter en toegankelijker, zelfs voor gebruikers met beperkte technische vaardigheden. Bovendien worden API's over het algemeen onderhouden door de website-eigenaar, wat de betrouwbaarheid van de gegevensstroom verhoogt in vergelijking met webscraping, waarbij de structuur van de website kan veranderen zonder voorafgaande kennisgeving, wat leidt tot inconsistenties in de gegevens. Een ander voordeel van API's is hun snelheid. In vergelijking met webscraping kunnen API's gegevens sneller leveren doordat ze directe toegang hebben tot de backend van de website, waardoor de latentie wordt verminderd en de totale efficiëntie van het verzamelproces wordt verbeterd.

Een ander voordeel van API's is hun snelheid. In vergelijking met webscraping kunnen API's gegevens sneller leveren doordat ze directe toegang hebben tot de backend van de website, waardoor de latentie wordt verminderd en de algehele efficiëntie van het verzamelproces wordt verbeterd.

\vspace{5mm} %5mm vertical space

Ondanks deze voordelen zijn er echter ook nadelen verbonden aan het gebruik van API's. Een van de belangrijkste beperkingen is de beschikbaarheid ervan. Niet alle websites bieden API's aan, dit limiteert de mogelijkheden voor gegevensverzameling , vooral bij het werken met minder bekende bronnen.
Daarnaast kunnen API's beperkte functionaliteit bieden, waarbij mogelijk niet alle benodigde gegevens beschikbaar zijn via de API. Dit kan gebruikers dwingen om toch andere methoden te gebruiken om aanvullende gegevens te verkrijgen, wat de complexiteit van het verzamelproces kan vergroten.
Een ander nadeel van het gebruik van API's is het vereiste authenticatieproces. Sommige API's vereisen dat gebruikers zich authentiseren alvorens ze toegang krijgen tot de API, wat extra stappen met zich meebrengt en de implementatie kan compliceren.

\subsection{Netwerkverkeersanalyse}

Netwerkverkeersanalyse, ook wel packet capture of network sniffing genoemd, is een techniek waarbij datapakketten die over een computernetwerk reizen worden geanalyseerd \autocite{Chapple2018}.  In tegenstelling tot traditionele webscraping methoden die de HTML-code van webpagina’s parsen, richt deze methode zich op de data die wordt uitgewisseld tussen de client en de server. Het analyseren van netwerkverkeer is efficiënter dan het parsen van HTML-code.  De
data die via netwerkverkeersanalyse wordt geëxtraheerd is vaak al gestructureerd en geformatteerd, waardoor het eenvoudiger te verwerken is.
Deze methode is in tegenstelling tot de traditionele manier veel flexibeler omdat er niet naar de
HTML-code wordt gekeken maar wel naar het verkeer en dus de data die verstuurd wordt. Dit wil
zeggen dat de scrapers nog steeds zullen werken
ook als de website geupdate wordt. Websites implementeren steeds vaker anti-scraping maatregelen om te voorkomen dat hun data wordt gescraped. Netwerkverkeersanalyse is minder gevoelig voor deze maatregelen, omdat het de data
extraheert voordat deze door de website wordt
geladen. De analyse van netwerkverkeer is
complexer dan de traditionele webscraping methoden. Het vereist meer technische kennis om
de data te onderscheppen,te interpreteren en te
verwerken. Soms versleutelen websites hun netwerkverkeer, waardoor het moeilijker of zelfs onmogelijk is om data te extraheren. Een aantal tools die hiervoor kan gebruikt worden is Wireshark,
dit is een opensource tool met een breed scala
aan funties voor het vastleggen van allerlei soorten netwerkverkeer. Ook de python libraries ’scapy’
en ’mitmproxy’ bieden krachtige tools voor het
analyseren en manipuleren van netwerkverkeer.

\section{Anti webscraping maatregelen}

\section{Netwerkanalyse tools}
\subsection{WireShark}
\subsection{Postman}

